# Diffusion_Model_Testing_Framework
Diffusion Model Testing Framework is a toolset for systematically testing diffusion-based generative models. It supports automated evaluation of image quality, prompt alignment, diversity, and failure cases. Designed for researchers, it helps ensure consistency, robustness, and reliability in model outputs.

# 🧪 Diffusion Model Testing Framework

A comprehensive and modular testing framework for systematically evaluating the performance and robustness of diffusion-based generative models such as Stable Diffusion, Imagen, and custom-trained architectures.

---

## 📌 Project Description

**Diffusion Model Testing Framework** is designed to test and validate image outputs generated by diffusion models. It supports automated pipelines for assessing prompt-image alignment, visual quality, diversity, and identifying failure cases like artifacts, hallucinations, and prompt mismatch. The framework is ideal for research, development, model comparison, and quality assurance in generative AI.

---

## 🚀 Features

- 📊 Evaluate image quality using CLIP similarity, SSIM, FID, and LPIPS
- 🔍 Detect and log failure cases (e.g., blank, blurry, mismatched)
- 🧠 Prompt-image semantic alignment scoring
- 🔁 Batch test pipelines across prompts, seeds, and models
- 🗃️ JSON/CSV results export for analysis
- 🔌 Easily plug in models via API or local inference script

---

## 🏗️ Tech Stack

- Python (evaluation scripts and test runners)
- OpenAI CLIP / BLIP for semantic similarity
- scikit-image, NumPy, PIL for image processing
- pandas for result handling
- (Optional) Flask or Streamlit for dashboard integration

---

## 📦 Installation

```bash
git clone https://github.com/yourusername/diffusion-model-testing-framework.git
cd diffusion-model-testing-framework
pip install -r requirements.txt

⚙️ Usage
 Run tests on a batch of generated images and prompts:
  python test_runner.py --images ./outputs/ --prompts prompts.json
 Generate a visual report:
  python generate_report.py --results results.csv

📁 Project Structure
diffusion-model-testing-framework/
├── test_runner.py
├── evaluators/
│   ├── clip_score.py
│   ├── ssim_fid.py
├── prompts/
│   └── prompts.json
├── outputs/
│   └── generated_images/
├── results/
│   └── scores.csv
├── utils/
│   └── image_loader.py
└── README.md

📊 Evaluation Metrics
 - CLIP Similarity – Measures semantic alignment with the prompt
 - SSIM / PSNR – Structural similarity between images
 - FID / LPIPS – Perceptual image quality
 - Failure Tags – Manual/automated labeling for blank, off-topic, or artifact images
