# Diffusion_Model_Testing_Framework
Diffusion Model Testing Framework is a toolset for systematically testing diffusion-based generative models. It supports automated evaluation of image quality, prompt alignment, diversity, and failure cases. Designed for researchers, it helps ensure consistency, robustness, and reliability in model outputs.

# ğŸ§ª Diffusion Model Testing Framework

A comprehensive and modular testing framework for systematically evaluating the performance and robustness of diffusion-based generative models such as Stable Diffusion, Imagen, and custom-trained architectures.

---

## ğŸ“Œ Project Description

**Diffusion Model Testing Framework** is designed to test and validate image outputs generated by diffusion models. It supports automated pipelines for assessing prompt-image alignment, visual quality, diversity, and identifying failure cases like artifacts, hallucinations, and prompt mismatch. The framework is ideal for research, development, model comparison, and quality assurance in generative AI.

---

## ğŸš€ Features

- ğŸ“Š Evaluate image quality using CLIP similarity, SSIM, FID, and LPIPS
- ğŸ” Detect and log failure cases (e.g., blank, blurry, mismatched)
- ğŸ§  Prompt-image semantic alignment scoring
- ğŸ” Batch test pipelines across prompts, seeds, and models
- ğŸ—ƒï¸ JSON/CSV results export for analysis
- ğŸ”Œ Easily plug in models via API or local inference script

---

## ğŸ—ï¸ Tech Stack

- Python (evaluation scripts and test runners)
- OpenAI CLIP / BLIP for semantic similarity
- scikit-image, NumPy, PIL for image processing
- pandas for result handling
- (Optional) Flask or Streamlit for dashboard integration

---

## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/diffusion-model-testing-framework.git
cd diffusion-model-testing-framework
pip install -r requirements.txt

âš™ï¸ Usage
 Run tests on a batch of generated images and prompts:
  python test_runner.py --images ./outputs/ --prompts prompts.json
 Generate a visual report:
  python generate_report.py --results results.csv

ğŸ“ Project Structure
diffusion-model-testing-framework/
â”œâ”€â”€ test_runner.py
â”œâ”€â”€ evaluators/
â”‚   â”œâ”€â”€ clip_score.py
â”‚   â”œâ”€â”€ ssim_fid.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ prompts.json
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ generated_images/
â”œâ”€â”€ results/
â”‚   â””â”€â”€ scores.csv
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ image_loader.py
â””â”€â”€ README.md

ğŸ“Š Evaluation Metrics
 - CLIP Similarity â€“ Measures semantic alignment with the prompt
 - SSIM / PSNR â€“ Structural similarity between images
 - FID / LPIPS â€“ Perceptual image quality
 - Failure Tags â€“ Manual/automated labeling for blank, off-topic, or artifact images
